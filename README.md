# mlhw5
Indu Mathi Kakarla  A simple PyTorch Transformer Encoder Block with multi-head attention, feed-forward layers, and LayerNorm. Processes inputs of shape (batch, seq_len, d_model) and outputs the same shape. Includes residual connections and configurable d_model and num_heads.
